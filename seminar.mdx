---
title: The Use of the Visual Genome Dataset for Multimodal Machine Learning
---

<img
  className="block dark:hidden"
  src="/images/hero-light.svg"
  alt="Hero Light"
/>
<img
  className="hidden dark:block"
  src="https://production-media.paperswithcode.com/datasets/Visual_Genome-0000000087-aaf04589_6fjxO1x.jpg"
  alt="Hero Dark"
/>

## Objectives

- To provide an overview of the Visual Genome dataset.
- To discuss the challenges and opportunities of using the Visual Genome dataset for multimodal machine learning.
- To present some research that has been done using the Visual Genome dataset.
- To identify some potential future research directions.

Here are some of the things you can do with the Visual Genome dataset:

**1. Image captioning:** This is the task of generating a natural language description of an image. You can use the Visual Genome dataset to train a model to generate captions for images.

**2. Visual question answering:** This is the task of answering questions about an image. You can use the Visual Genome dataset to train a model to answer questions about images.

**3. Object detection:** This is the task of detecting objects in an image. You can use the Visual Genome dataset to train a model to detect objects in images.

**4. Relationship detection:** This is the task of detecting relationships between objects in an image. You can use the Visual Genome dataset to train a model to detect relationships between objects in images.

**5. Scene understanding:** This is the task of understanding the overall scene in an image. You can use the Visual Genome dataset to train a model to understand the overall scene in images.

## Datasets

Below some datasets are filtered out of 8440 datasets:

- Images 2420
- Text 2297
- Question Answering 349
- Object detection 241
- Image classification 225

... and many more

## Research papers

Below listed some research papers that uses Visual Genome datasets to create and train the machine learning model

<Card
  title="GIT: A Generative Image-to-text Transformer"
  href="https://paperswithcode.com/paper/git-a-generative-image-to-text-transformer"
>
  In this paper, design and train a Generative Image-to-text Transformer, GIT,
  to unify vision-language tasks such as image/video captioning and question
  answering.
</Card>

<Card
  title="You can find all papers here"
  icon="link"
  href="https://paperswithcode.com/dataset/visual-genome"
>
  Visual Genome contains Visual Question Answering data in a multi-choice
  setting. It consists of 101,174 images from MSCOCO with 1.7 million QA pairs,
  17 questions per image on average
</Card>
